# BayesianOptimization.jl å®˜æ–¹APIä½¿ç”¨æŒ‡å—

## ğŸ“– æ¦‚è¿°

BayesianOptimization.jl æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´å¶æ–¯ä¼˜åŒ–åº“ï¼ŒåŸºäºé«˜æ–¯è¿‡ç¨‹çš„ä»£ç†æ¨¡å‹æ¥ä¼˜åŒ–æ˜‚è´µçš„é»‘ç®±å‡½æ•°ã€‚

## ğŸš€ æ ¸å¿ƒç»„ä»¶

### 1. ä¸»è¦APIç»“æ„

```julia
# åŸºæœ¬å·¥ä½œæµ
opt = BOpt(func, model, acquisition, modeloptimizer, lowerbounds, upperbounds; kwargs...)
result = boptimize!(opt)
```

### 2. ä¸»è¦å¯¼å‡ºç±»å‹å’Œå‡½æ•°

**æ ¸å¿ƒå‡½æ•°ï¼š**
- `BOpt` - è´å¶æ–¯ä¼˜åŒ–å™¨æ„é€ å‡½æ•°
- `boptimize!` - æ‰§è¡Œä¼˜åŒ–
- `optimize` - å¿«é€Ÿä¼˜åŒ–æ¥å£ï¼ˆä½¿ç”¨é»˜è®¤å‚æ•°ï¼‰

**é‡‡é›†å‡½æ•°ï¼š**
- `ExpectedImprovement()` - æœŸæœ›æ”¹è¿›
- `ProbabilityOfImprovement()` - æ”¹è¿›æ¦‚ç‡
- `UpperConfidenceBound()` - ä¸Šç½®ä¿¡ç•Œ
- `ThompsonSamplingSimple()` - ç®€å•æ±¤æ™®æ£®é‡‡æ ·
- `MutualInformation()` - äº’ä¿¡æ¯

**æ¨¡å‹ä¼˜åŒ–å™¨ï¼š**
- `MAPGPOptimizer` - æœ€å¤§åéªŒä¼°è®¡ä¼˜åŒ–å™¨
- `NoModelOptimizer` - ä¸ä¼˜åŒ–æ¨¡å‹è¶…å‚æ•°

**å…¶ä»–å·¥å…·ï¼š**
- `ScaledSobolIterator`, `ScaledLHSIterator` - åˆå§‹åŒ–é‡‡æ ·
- `Min`, `Max` - ä¼˜åŒ–æ–¹å‘
- `Silent`, `Timings`, `Progress` - è¯¦ç»†ç¨‹åº¦
- `maxduration!`, `maxiterations!` - åŠ¨æ€è°ƒæ•´

## ğŸ”§ è¯¦ç»†ç”¨æ³•

### BOpt æ„é€ å‡½æ•°

```julia
BOpt(func, model, acquisition, modeloptimizer, lowerbounds, upperbounds;
     sense = Max,                    # ä¼˜åŒ–æ–¹å‘ (Max/Min)
     maxiterations = 10^4,           # æœ€å¤§è¿­ä»£æ¬¡æ•°
     maxduration = Inf,              # æœ€å¤§è¿è¡Œæ—¶é—´
     acquisitionoptions = NamedTuple(), # é‡‡é›†å‡½æ•°ä¼˜åŒ–é€‰é¡¹
     repetitions = 1,                # æ¯ä¸ªç‚¹çš„é‡å¤è¯„ä¼°æ¬¡æ•°
     verbosity = Progress,           # è¾“å‡ºè¯¦ç»†ç¨‹åº¦
     initializer_iterations = 5*length(lowerbounds), # åˆå§‹é‡‡æ ·ç‚¹æ•°
     initializer = ScaledSobolIterator(...))  # åˆå§‹åŒ–é‡‡æ ·å™¨
```

### MAPGPOptimizer è¯¦ç»†é…ç½®

```julia
# å…³é”®ï¼škernbounds çš„æ­£ç¡®æ ¼å¼
MAPGPOptimizer(
    every = 20,                     # æ¯20æ­¥ä¼˜åŒ–ä¸€æ¬¡è¶…å‚æ•°
    noisebounds = [-4, 3],          # å¯¹æ•°å™ªå£°è¾¹ç•Œ [ä¸‹ç•Œ, ä¸Šç•Œ]
    kernbounds = [
        [-3*ones(d); -3],           # ä¸‹ç•Œï¼š[æ ¸å‚æ•°ä¸‹ç•Œ..., å¯¹æ•°ä¿¡å·æ–¹å·®ä¸‹ç•Œ]
        [4*ones(d); 3]              # ä¸Šç•Œï¼š[æ ¸å‚æ•°ä¸Šç•Œ..., å¯¹æ•°ä¿¡å·æ–¹å·®ä¸Šç•Œ]
    ],
    maxeval = 100                   # è¶…å‚æ•°ä¼˜åŒ–çš„æœ€å¤§è¯„ä¼°æ¬¡æ•°
)
```

**é‡è¦è¯´æ˜ï¼š**
- `kernbounds` æ˜¯ `[ä¸‹ç•Œå‘é‡, ä¸Šç•Œå‘é‡]` æ ¼å¼
- å¯¹äº `SEArd` æ ¸ï¼šéœ€è¦ `d+1` ä¸ªå‚æ•°ï¼ˆdä¸ªé•¿åº¦å°ºåº¦ + 1ä¸ªä¿¡å·æ–¹å·®ï¼‰
- è¾¹ç•Œå¿…é¡»æ»¡è¶³ `ä¸‹ç•Œ[i] <= ä¸Šç•Œ[i]`

### é«˜æ–¯è¿‡ç¨‹æ¨¡å‹è®¾ç½®

```julia
using GaussianProcesses

# åˆ›å»ºå¼¹æ€§é«˜æ–¯è¿‡ç¨‹æ¨¡å‹
model = ElasticGPE(
    d,                              # è¾“å…¥ç»´åº¦
    mean = MeanConst(0.0),         # å‡å€¼å‡½æ•°
    kernel = SEArd(zeros(d), 0.0), # æ ¸å‡½æ•°ï¼šè‡ªåŠ¨ç›¸å…³ç¡®å®šçš„å¹³æ–¹æŒ‡æ•°æ ¸
    logNoise = -2.0,               # å¯¹æ•°å™ªå£°
    capacity = 3000                # å®¹é‡
)

# é¢„åŠ è½½æ•°æ®çš„æƒ…å†µ
X_init = # dÃ—n çŸ©é˜µï¼ˆç‰¹å¾ç»´åº¦Ã—æ ·æœ¬æ•°ï¼‰
y_init = # nç»´å‘é‡
gp = GP(X_init, y_init, MeanConst(0.0), SEArd(ones(d), 0.0))
```

## ğŸ“‹ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šåŸºæœ¬ç”¨æ³•

```julia
using BayesianOptimization, GaussianProcesses

# ç›®æ ‡å‡½æ•°
f(x) = -(x[1] - 2)^2 - (x[2] + 1)^2 + 5

# æ¨¡å‹
model = ElasticGPE(2, mean = MeanConst(0.0), 
                   kernel = SEArd([0.0, 0.0], 0.0), 
                   logNoise = -2.0)

# æ¨¡å‹ä¼˜åŒ–å™¨ - æ­£ç¡®çš„kernboundsæ ¼å¼
modeloptimizer = MAPGPOptimizer(
    every = 10,
    noisebounds = [-4, 3],
    kernbounds = [[-2, -2, -3], [3, 3, 2]], # [x1_scale, x2_scale, signal_var]
    maxeval = 100
)

# ä¼˜åŒ–å™¨
opt = BOpt(f, model, ExpectedImprovement(), modeloptimizer,
           [-5.0, -5.0], [5.0, 5.0],
           sense = Max,
           maxiterations = 50,
           verbosity = Progress)

# æ‰§è¡Œä¼˜åŒ–
result = boptimize!(opt)
println("æœ€ä¼˜è§£: ", result.observed_optimizer)
println("æœ€ä¼˜å€¼: ", result.observed_optimum)
```

### ç¤ºä¾‹2ï¼šé¢„åŠ è½½æ•°æ®çš„çƒ­å¯åŠ¨

```julia
# å·²æœ‰çš„æ•°æ®ç‚¹
X_init = [[-1.0, 1.0], [0.0, 0.0], [2.0, -1.0]]  # åˆå§‹ç‚¹åˆ—è¡¨
y_init = [f(x) for x in X_init]                   # å¯¹åº”çš„å‡½æ•°å€¼

# è½¬æ¢ä¸ºGPéœ€è¦çš„æ ¼å¼
X_matrix = hcat(X_init...)  # 2Ã—3 çŸ©é˜µ
y_vector = Vector{Float64}(y_init)

# åˆ›å»ºé¢„åŠ è½½çš„GP
gp = GP(X_matrix, y_vector, MeanConst(0.0), SEArd([1.0, 1.0], 0.0))

# ä¼˜åŒ–å™¨ï¼ˆè®¾ç½® initializer_iterations = 0ï¼‰
opt = BOpt(f, gp, UpperConfidenceBound(), 
           NoModelOptimizer(),  # ä½¿ç”¨å›ºå®šè¶…å‚æ•°
           [-5.0, -5.0], [5.0, 5.0],
           sense = Max,
           maxiterations = 30,
           initializer_iterations = 0,  # ä¸è¿›è¡Œé¢å¤–åˆå§‹åŒ–
           verbosity = Progress)

result = boptimize!(opt)
```

### ç¤ºä¾‹3ï¼šä¸€ç»´ä¼˜åŒ–

```julia
# ä¸€ç»´å‡½æ•°
f_1d(x) = -(x[1] - 3)^2 + 5

# ä¸€ç»´åˆå§‹æ•°æ®
X_1d = reshape([1.0, 2.0, 4.0], 1, 3)  # 1Ã—3 çŸ©é˜µ
y_1d = [f_1d([x]) for x in [1.0, 2.0, 4.0]]

# ä¸€ç»´GP
gp_1d = GP(X_1d, y_1d, MeanConst(0.0), SEArd([1.0], 0.0))

# ä¸€ç»´ä¼˜åŒ–å™¨
opt_1d = BOpt(f_1d, gp_1d, ExpectedImprovement(),
              MAPGPOptimizer(every = 5, 
                           noisebounds = [-4, 3],
                           kernbounds = [[-2, -3], [3, 2]]), # [length_scale, signal_var]
              [0.0], [6.0],
              sense = Max,
              maxiterations = 20,
              initializer_iterations = 0)

result_1d = boptimize!(opt)
```

## âš™ï¸ é«˜çº§é…ç½®

### é‡‡é›†å‡½æ•°é€‰é¡¹

```julia
# é‡‡é›†å‡½æ•°ä¼˜åŒ–é…ç½®
acquisitionoptions = (
    method = :LD_LBFGS,    # NLoptä¼˜åŒ–æ–¹æ³•
    restarts = 10,         # éšæœºé‡å¯æ¬¡æ•°
    maxeval = 2000,        # æœ€å¤§è¯„ä¼°æ¬¡æ•°
    maxtime = 0.1          # æœ€å¤§æ—¶é—´é™åˆ¶
)

opt = BOpt(f, model, acquisition, modeloptimizer, bounds...,
           acquisitionoptions = acquisitionoptions)
```

### ä¸åŒé‡‡é›†å‡½æ•°çš„ç‰¹ç‚¹

```julia
# æœŸæœ›æ”¹è¿›ï¼ˆæœ€å¸¸ç”¨ï¼‰
ExpectedImprovement()

# ä¸Šç½®ä¿¡ç•Œï¼ˆé€‚åˆæ¢ç´¢ï¼‰
UpperConfidenceBound(scaling = BrochuBetaScaling(0.1), Î²t = 1.0)

# æ”¹è¿›æ¦‚ç‡ï¼ˆä¿å®ˆï¼‰
ProbabilityOfImprovement()

# æ±¤æ™®æ£®é‡‡æ ·ï¼ˆéšæœºï¼‰
ThompsonSamplingSimple()

# äº’ä¿¡æ¯ï¼ˆç†è®ºæœ€ä¼˜ï¼‰
MutualInformation()
```

## ğŸ” å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 1. kernbounds è¾¹ç•Œé”™è¯¯
**é”™è¯¯**: `ArgumentError("invalid NLopt arguments: bounds 1 fail -1 <= 1 <= -2")`

**è§£å†³**: ç¡®ä¿ä¸‹ç•Œ <= ä¸Šç•Œ
```julia
# é”™è¯¯
kernbounds = [[-1, 3], [-2, 2]]  # -2 < 3 è¿åäº†ç¬¬äºŒä¸ªå‚æ•°çš„è¾¹ç•Œ

# æ­£ç¡®
kernbounds = [[-2, -3], [3, 2]]  # æ‰€æœ‰ä¸‹ç•Œéƒ½å°äºå¯¹åº”ä¸Šç•Œ
```

### 2. GP æ•°æ®æ ¼å¼
```julia
# GPéœ€è¦çš„æ•°æ®æ ¼å¼
X = hcat(points...)     # dÃ—n çŸ©é˜µï¼ˆç‰¹å¾ç»´åº¦Ã—æ ·æœ¬æ•°ï¼‰
y = Vector{Float64}(values)  # nç»´å‘é‡

# ä¸æ˜¯ nÃ—d çŸ©é˜µï¼
```

### 3. é¿å…è¶…å‚æ•°ä¼˜åŒ–é—®é¢˜
```julia
# å¦‚æœMAPGPOptimizeræœ‰é—®é¢˜ï¼Œä½¿ç”¨å›ºå®šå‚æ•°
opt = BOpt(f, gp, acquisition, NoModelOptimizer(), bounds...)
```

## ğŸ“š å‚è€ƒèµ„æ–™

- [GitHub ä»“åº“](https://github.com/jbrea/BayesianOptimization.jl)
- [å®˜æ–¹æ–‡æ¡£](https://jbrea.github.io/BayesianOptimization.jl/dev/)
- Brochu et al. (2010): "A Tutorial on Bayesian Optimization"

## ğŸ’¡ æœ€ä½³å®è·µ

1. **å¼€å§‹ç®€å•**: å…ˆç”¨ `NoModelOptimizer` æµ‹è¯•åŸºæœ¬åŠŸèƒ½
2. **æ•°æ®æ ¼å¼**: ç¡®ä¿ X æ˜¯ dÃ—n æ ¼å¼ï¼Œy æ˜¯å‘é‡
3. **è¾¹ç•Œè®¾ç½®**: ä»”ç»†æ£€æŸ¥ `kernbounds` çš„è¾¹ç•Œå…³ç³»
4. **é‡‡é›†å‡½æ•°**: `ExpectedImprovement` é€šå¸¸æ˜¯æœ€å¥½çš„èµ·ç‚¹
5. **è°ƒè¯•**: ä½¿ç”¨ `verbosity = Progress` ç›‘æ§ä¼˜åŒ–è¿‡ç¨‹

## ğŸ¯ æ€»ç»“

BayesianOptimization.jl æä¾›äº†å®Œæ•´çš„è´å¶æ–¯ä¼˜åŒ–åŠŸèƒ½ï¼Œå…³é”®æ˜¯ï¼š
- æ­£ç¡®è®¾ç½®æ•°æ®æ ¼å¼ï¼ˆdÃ—nçŸ©é˜µï¼‰
- åˆç†é…ç½® `kernbounds`ï¼ˆä¸‹ç•Œ <= ä¸Šç•Œï¼‰
- é€‰æ‹©åˆé€‚çš„é‡‡é›†å‡½æ•°å’Œæ¨¡å‹ä¼˜åŒ–å™¨
- ä»ç®€å•é…ç½®å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚æ€§
